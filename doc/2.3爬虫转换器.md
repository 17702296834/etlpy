# 爬虫转换器

etlpy针对爬虫进行了特定的优化，提供了一系列算子

## 访问网页数据：get,post

`url.set('www.cnblogs.com').cp('_:content').get()`

将先将url保存在url列，访问web,再将获取的content保存回p列。

如果想发送post请求，post数据可以以字典形式放在某列，作为参数传给post算子，如:

`p.set({'a':1,'b':2}).url.set('www.baidu.com').post('[p]')`

> 此处使用了括号表达式

代理的使用方法，参考本文最下方有关代理的相关内容


## tree,xpath,pyq

通过get,post方法获取的是原始的html文本，通过tree,可以将其加载为基于lxml的node节点。

之后，就可以通过xpath和pyquery获取html节点里的数据了。

例如: `url.set('www.cnblogs.com').tree().xpath('//div[2]')[0]`

xpath, pyq算子能分别接受xpath表达式和jquery表达式，得到的结果是满足条件的算子的数组。

上面可以省略tree，因为xpath,pyq会检查输入的目标列的值，如果为字符串会自动解析为tree。 提前使用tree是为了满足需要从html中多次解析数据的需要，例如：

`url.set('www.cnblogs.com').get().tree().cp('_:d1').xpath('//div[2]').cp('url:d2').xpath('//p[2]')`

这样html只会被解析一次(一直保存在url列中)。否则会被解析多次，考虑到解析时很消耗性能的，因此尽量显式使用tree算子。


> xpath和jquery语法请参考相关文档

## html和text

对于node节点，你可以调用html算子text算子，分别获取html或节点的text信息。因此，html()等价于dump('html'),tree()等价于load('html'),它们互为逆操作。

> 当输入列为其他类型时，text算子会对其执行str()操作。


## 自动嗅探和搜索

由于编写xpath依然费时费力，etlpy提供了智能的数据探测器，在tree算子之后，你可以使用detect算子，自动输出最佳的列表数据，并打印出对应的etlpy代码。例如：

`url.set('www.cnblogs.com').tree().detect()`


还可以在tree算子之后，执行search,参数为关键字，该算子会输出对应的xpath.


## 下载文件

dl算子用于下载文件。目标列保存超链接，算子参数为要保存的路径，例如:

`path.set('save_path').url.set('url').dl('[path]')`

TODO: 如果要加入其它cookie，则。。。。



## 使用代理

使用代理可以防止爬虫被网页拦截，此乃杀人越货必备佳品。

在流中，可能会使用多个get/post算子去访问同一个网站，因此对每个get,post单独设置代理就变得非常繁琐了。因此可以在工程的环境(env)中设置代理管理器，该管理器会拦截get/post/dl算子的执行，为其注入代理的代码。

